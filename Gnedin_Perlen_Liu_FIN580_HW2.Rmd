---
title: "Gnedin_Perlen_Liu_FIN580_HW2"
author: "Gnedin_Perlen_Liu"
date: "March 8, 2017"
output: pdf_document
---

```{r,tidy=TRUE}
#Setup 
library(xlsx)
library(forecast)
library(tseries)
library(stats)
library(car)
library(glmnet)
library(miscTools)
library(Metrics)
library(knitr)
library(class)
library(vars)
library(BigVAR)
library(orderedLasso)
library(BioPhysConnectoR)
#Set seed
set.seed(1560)

```

Read in data. There might some days the price remains constant. You should check the data and delete those days, since the estimate of volatility will be zero
```{r,tidy=TRUE}
#Get data
load_data_from_scratch=FALSE

file_names=dir(path="~/Desktop/Grad_School/2016-2017_Spring/Amin/Homework/HW 2/FIN580")
setwd("~/Desktop/Grad_School/2016-2017_Spring/Amin/Homework/HW 2/FIN580")
#file_names=dir(path="~/Desktop/Grad_School/2016-2017_Spring/Amin/Homework/HW 2/Data")
#setwd("~/Desktop/Grad_School/2016-2017_Spring/Amin/Homework/HW 2/Data")
period=288 #Only using daily vols for this assignment

if(load_data_from_scratch==TRUE){
 for(i in 1:length(file_names)){
  temp=read.csv(file_names[i],header = TRUE)
  n=ncol(temp)
  if(i==1){
    data=temp[,n]
  }
  else{
    data=cbind(data,temp[,n])
  }
 } 
  
#Change data to log returns
temp=diff(log(data),lag=period)
data=data[-1,]
data=temp
}


#Save data to .RData file 
if(load_data_from_scratch==FALSE){
  load("data.RData")
}
save(data,file="data.RData")

```

Get volatilities, annualize, and take the log. Fix windows.
```{r,tidy=TRUE}
period=288 #Only using daily vols for this assignment
get_vol=function(time_series,start_i,end_i){
  use_data=time_series[start_i:end_i,]
  sds=apply(use_data,2,sd)*sqrt(period*252)
  return(sds)
}
vols=data.frame(matrix(NA,ncol = ncol(data),nrow=nrow(data)/period))
vols=sapply(seq(1,nrow(data)-period,period),function(x) {get_vol(data,x,x+288)})
#Transpose to keep columns and currencies
vols=t(vols)
#Change to log vols
vols=log(vols)


save(vols,file="vols.RData")
load("vols.RData")

```

Here we split our data into training and test. Since we use VAR models we will split our data sequentially. We will aim to use 2/3 of the data for the training set, and 1/3 for the testing.
```{r,tidy=TRUE}
split=floor(nrow(vols)*(3.5/5))
train=vols[1:split,]
test=vols[(split+1):nrow(vols),]
save(train,file="train.RData")
save(test,file="test.RData")
```


General functions to be used across models. 
```{r,tidy=TRUE}
#Since the R lag function doesn't pad with NAs - we write our own lag function
lagpad = function(x, k) {
    if (!is.vector(x)) 
        stop('x must be a vector')
    if (!is.numeric(x)) 
        stop('x must be numeric')
    if (!is.numeric(k))
        stop('k must be numeric')
    if (1 != length(k))
        stop('k must be a single number')
    c(rep(NA, k), x)[1 : length(x)] 
}
#Get lagged data for autoreggressive models
get_lagged_data=function(time_series,max_lag){
  xs=matrix(NA,nrow=length(time_series),ncol=max_lag)
  xs[,1]=lagpad(time_series,1)
  if(max_lag>1){
    xs[,2]=lagpad(time_series,2)
  }
  if(max_lag>2){
    xs[,3]=lagpad(time_series,3)
  }
  return(na.omit(xs))
}
```

VAR model (non lasso).
```{r,tidy=TRUE}
ps=c(1,2,3)
colnames(train)=seq(1,ncol(train),1) #VAR cares about col names for some reason
fit_VAR=function(p){
  model=VAR(train,p=p)
  pred=predict(model,n.ahead=nrow(train))
  all=pred[[1]]
  data=all[[1]][,1]
  for(i in 2:ncol(test)){
    temp=all[[i]][,1]
    data=cbind(data,temp)
  }
  errs=(data-train)^2
  return(errs)
}

var_mses=list()
for(i in 1:length(ps)){
  var_mses[[i]]=fit_VAR(i)
}

#Plotting
plot_data=matrix(NA,nrow=3,ncol=ncol(test))
for(i in 1:ncol(test)){
  plot_data[1,]=mean(var_mses[[1]][,i])
  plot_data[2,]=mean(var_mses[[2]][,i])
  plot_data[3,]=mean(var_mses[[3]][,i])
}

matplot(plot_data,main="VAR model MSE by lag",xlab="Lag",ylab="MSE")
legend("bottomright",legend=1:9,col=1:9,pch=1)
dev.copy(pdf,"Var model MSE by lag")
dev.off()

```
LASSO described in [1].  You are expected to run one regression with the whole training history ( rst 3.5 years) for each λ. Run the regressions with different λs and plot the  in sample  Mean Square Error (MSE) with respect to the λ. Use the best λ (corresponds to the lowest MSE) in the test sample to report the  nal evaluation numbers.
```{r,tidy=TRUE}
#Available lags
ps=c(1,2,3)

#VAR lasso
lams_opt_VAR_l=c()
fit_VAR_lasso=function(p){
  model=constructModel(data.matrix(train),p=p,"Basic",gran=c(50,10),h=1)
  res=cv.BigVAR(model)
  o_lam=res@OptimalLambda
  mse_in=res@InSampMSFE
  name=paste("VAR Lasso Results Lag",p)
  #Plot and save as pdf
  plot(res)
  title(main = name)
  #Save as pdf 
  dev.copy(pdf,paste(name,".pdf",sep=""))
  dev.off()
  return(list(o_lam,mse_in))
}

for(i in 1:length(ps)){
  temp=fit_VAR_lasso(i)
  lams_opt_VAR_l=append(lams_opt_VAR_l,temp[[1]])
}

#Expanding window fits of VAR lasso models using the optimal lambda to get 1 step predictions
#Prediction function - returns MSE for 1 prediction over all currencies
pred_var_lasso=function(data,lam,p,ans){
  model=constructModel(data.matrix(data),p=p,"Basic",ownlambdas=TRUE,gran=lam,h=1,verbose=FALSE)
  res=cv.BigVAR(model)
  pred=t(predict(res,1))
  return(mean((matrix(ans,nrow=1)-pred)^2))
}

#Returns more detailed statistics - currency by currency
pred_var_lasso_det=function(data,lam,p,ans){
  model=constructModel(data.matrix(data),p=p,"Basic",ownlambdas=TRUE,gran=lam,h=1,verbose=FALSE)
  res=cv.BigVAR(model)
  pred=t(predict(res,1))
  return((matrix(ans,nrow=1)-pred)^2)
}




#Non detailed
temp=seq(1,nrow(test)-2,1)
mses_var_lasso=data.frame(matrix(NA,nrow=(length(temp)+1),ncol=3))

for(i in 1:length(ps)){
  i=3
  lam=lams_opt_VAR_l[i]
  mses_var_lasso[1,i]=pred_var_lasso(train,lam,i,test[1,])
  mses_var_lasso[2:nrow(mses_var_lasso),i]=sapply(temp,function(x){print(x) 
    pred_var_lasso(rbind(train,test[1:(x),]),lam,i,test[x+1,])})
}

save(mses_var_lasso,file="mses_var_lasso.RData")

#Detailed
mses_var_lasso_det=data.frame(matrix(NA,nrow=(length(temp)+1),ncol=3*9))
for(i in 1:length(ps)){
  lam=lams_opt_VAR_l[i]
  mses_var_lasso[1,i:(3*i)]=pred_var_lasso(train,lam,i,test[1,])
  mses_var_lasso[2:nrow(mses_var_lasso),i:(3*i)]=sapply(temp,function(x){print(x) 
    pred_var_lasso(rbind(train,test[1:(x),]),lam,i,test[x+1,])})
}
save(mses_var_lasso_det,file="mses_var_lasso_det.RData")


```

$\textbf{Ordered lasso implementation}$
Here we fit the ordered LASSO for extra credit. 
```{r,tidy=TRUE}

#Fits lasso based on lag 
#Can also do ordered lasso based on order parameter
fit_lasso=function(col_i,max_lag,lam,order){
  ts=train[,col_i]
  ts=ts[which(ts!=0)]
  y=matrix(ts[(max_lag+1):length(ts)])
  x=get_lagged_data(ts,max_lag)
  model=orderedLasso(x,y,lambda = lam,strongly.ordered = order)
  pred_train=predict(model,x)$yhat
  new_ts=test[,col_i]
  new_ts=new_ts[which(new_ts!=0)]
  new_y=matrix(new_ts[(max_lag+1):length(new_ts)])
  new_x=get_lagged_data(new_ts,max_lag)
  pred_test=predict(model,new_x)$yhat
  return(list(mse(pred_train,y),mse(pred_test,new_y)))
}

#Getting range of lambdas
lam_1=0
#Get lam_L - maximum lambda, shrinks all parameters to 0 - based on most complex model (p=3)
currs=seq(1,ncol(train),1)
max_lag=max(ps)
get_max_L=function(col_i){
  ts=train[,col_i]
  ts=ts[which(ts!=0)]
  y=matrix(ts[(max_lag+1):length(ts)])
  x=get_lagged_data(ts,3)
  cv=cv.glmnet(x,y)
  return(cv$lambda[1])
}
max_L=sapply(currs,get_max_L)
lam_L=max(max_L)
lams=seq(0,lam_L,length.out = 10)

#Mses is a list - train first, test second
populate_tables=function(p,order){
  mse_train=matrix(NA,ncol=ncol(vols),nrow=length(lams))
  mse_test=matrix(NA,ncol=ncol(vols),nrow=length(lams))
  for(i in 1:ncol(mse_train)){
  temp=sapply(lams,function(x) fit_lasso(i,p,x,order))
  mse_train[,i]=matrix(unlist(temp[1,]),ncol=1)
  mse_test[,i]=matrix(unlist(temp[2,]),ncol=1)
  }
  return(list(mse_train,mse_test))
}

#Getting all MSE tables for ordered lasso - note we exclude 1lag as this is not a VAR model
for(i in 2:length(ps)){
  p=ps[i]
  name_train=paste("train_ordered_",p,sep="")
  name_test=paste("test_ordered_",p,sep="")
  temp=populate_tables(p,TRUE)
  assign(name_train,temp[1])
  assign(name_test,temp[2])
}
```

Plot MSEs by lambda for ordered Lasso.
```{r,tidy=TRUE}
#Get average MSEs by lambda (for plotting)
ave_mse_train=matrix(1,nrow=length(lams),ncol=length(ps))
ave_mse_test=matrix(1,nrow=length(lams),ncol=length(ps))

for(i in 1:length(ps)){
  name_train=paste("train_ordered_",p,sep="")
  name_test=paste("test_ordered_",p,sep="")
  use_train=matrix(unlist(get(name_train)),ncol=ncol(vols))
  use_test=matrix(unlist(get(name_test)),ncol=ncol(vols))
  ave_mse_train[,i]=rowMeans(use_train)
  ave_mse_test[,i]=rowMeans(use_test)
}

#PLotting
matplot(lams,ave_mse_train,type=c("b"),pch=1,col=1:3,main="Training MSEs by lambda",xlab="Lambdas",ylab="Ave MSE over all currencies")
legend("topright",legend=1:3,col=1:3,pch=1)
matplot(lams,ave_mse_test,type=c("b"),pch=1,col=1:3,main="Testing MSEs by lambda",xlab="Lambdas",ylab="Ave MSE over all currencies")
legend("topright",legend=1:3,col=1:3,pch=1)

```


KNN method we’ve covered during class. Features and Labels First, you should use today’s vol t as the 1-dimensional feature space, which means $x_t$ = t. The label is just what you want to forecast, which means $y_t$ = t+1. Note that you can choose different k for KNN, such as k = {1, 3, 5, 10, 20, 50}.
Training Sample: For each day/week/month, we can either use expanding window, that means use all the data up to that date as training sample to find the k-nearest points, or you can use rolling window, that means, fix the look-back window when training your model, say for each day, you can always lookback
for 2 years as training sample.

KNN regression Based on your feature space and training sample, once you found your k-nearest points to
$x_t$ = t, you can use the KNN regression we’ve covered in class (details can be found in course notes) to build a forecast for $y_t$ = t+1.
```{r,tidy=TRUE}
ks=c(1,3,5,10,20)
lookback_knn=period*252*2 #Two years
accuracy_knn=c()
knn_prediction=function(i,k){
  train_ts=train[i-lookback_knn-1:i-1]
  train_labels=train[i-lookback_knn:i]
  knn=knn(train_ts,test,train_labels,k=k)
  return(mean(knn==test[-1]))
}


```