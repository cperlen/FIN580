---
title: "Gnedin_Perlen_Liu_FIN580_HW2"
author: "Gnedin_Perlen_Liu"
date: "March 8, 2017"
output: pdf_document
---

```{r,tidy=TRUE}
#Setup 
library(xlsx)
library(forecast)
library(tseries)
library(stats)
library(car)
library(glmnet)
library(miscTools)
library(Metrics)
library(knitr)
library(class)
library(BioPhysConnectoR)
#Set seed
set.seed(666)

```

Read in data. There might some days the price remains constant. You should check the data and delete those days, since the estimate of volatility will be zero
```{r,tidy=TRUE}
#Get data
file_names=dir(path="~/Desktop/Grad_School/2016-2017_Spring/Amin/Homework/HW 2/Data")
setwd("~/Desktop/Grad_School/2016-2017_Spring/Amin/Homework/HW 2/Data")

for(i in 1:length(file_names)){
  temp=read.csv(file_names[i],header = TRUE)
  n=ncol(temp)
  if(i==1){
    data=temp[,n]
  }
  else{
    data=cbind(data,temp[,n])
  }
}

#Change data to log returns
temp=diff(log(data))
data=data[-1,]
data=temp

#TBU
remove_constants=function(time_series){
  new_series=time_series
  for(i in 1:length(time_series)){
    if(new_series[i+1]==new_series[i]){
      
    }
  }
}
```

Get volatilities, annualize, and take the log. Fix windows.
```{r,tidy=TRUE}
period=288 #Only using daily vols for this assignment
lookback_lasso=100 #Almost half a year - per assignment recomendation 
get_vol=function(time_series,start_i,end_i){
  use_data=time_series[start_i:end_i,]
  sds=apply(use_data,2,sd)*sqrt(period*252)
  return(sds)
}
vols=data.frame(matrix(NA,ncol = ncol(data),nrow=nrow(data)/period))
vols=sapply(seq(1,nrow(data),period),function(x) {get_vol(data,x,x+288)})
#Transpose to keep columns and currencies
vols=t(vols)
#Change to log vols
vols=log(vols)
```

Here we split our data into training and test. Since we use VAR models we will split our data sequentially. We will aim to use 2/3 of the data for the training set, and 1/3 for the testing.
```{r,tidy=TRUE}
split=floor(nrow(vols)*(2/3))
train=vols[1:split,]
test=vols[(split+1):nrow(vols),]
```

LASSO described in [1]. You should also use log volatility instead of volatility to address the negativity problem in regression. Compare the two results. You should divide your data into training and test sample, so that you can find the optimized in training set and use it for the test sample.
```{r,tidy=TRUE}
lam_1=0
#Get lam_L
cv=cv.glemnet(x,y)
lam_L=cv$lambda[1] #Lambda L is an estimate of the sparsity parameter that shrinks all parameters to 0
lasso_prediction=function(lam,lookback,index,time_series){
  x_data=time_series[index-lookback-1:index-1]
  y_data=time_series[index-lookback:index]
}

```


KNN method we’ve covered during class. Features and Labels First, you should use today’s vol t as the 1-dimensional feature space, which means $x_t$ = t. The label is just what you want to forecast, which means $y_t$ = t+1. Note that you can choose different k for KNN, such as k = {1, 3, 5, 10, 20, 50}.
Training Sample: For each day/week/month, we can either use expanding window, that means use all the data up to that date as training sample to find the k-nearest points, or you can use rolling window, that means, fix the look-back window when training your model, say for each day, you can always lookback
for 2 years as training sample.

KNN regression Based on your feature space and training sample, once you found your k-nearest points to
$x_t$ = t, you can use the KNN regression we’ve covered in class (details can be found in course notes) to build a forecast for $y_t$ = t+1.
```{r,tidy=TRUE}
ks=c(1,3,5,10,20)
lookback_knn=period*252*2 #Two years
accuracy_knn=c()
knn_prediction=function(i,k){
  train_ts=train[i-lookback_knn-1:i-1]
  train_labels=train[i-lookback_knn:i]
  knn=knn(train_ts,test,train_labels,k=k)
  return(mean(knn==test[-1]))
}


```