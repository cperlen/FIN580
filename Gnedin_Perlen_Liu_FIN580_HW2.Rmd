---
title: "Gnedin_Perlen_Liu_FIN580_HW2"
author: "Gnedin_Perlen_Liu"
date: "March 8, 2017"
output: pdf_document
---
This is the latest version. 
```{r,tidy=TRUE}
#Setup 
library(gridExtra)
library(xlsx)
library(forecast)
library(tseries)
library(stats)
library(car)
library(glmnet)
library(miscTools)
library(Metrics)
library(knitr)
library(class)
library(vars)
library(BigVAR)
library(orderedLasso)
library(BioPhysConnectoR)

#Set seed
set.seed(1560)

```

Read in data. There might some days the price remains constant. You should check the data and delete those days, since the estimate of volatility will be zero.  

Note: You will have to change the paths to read in on your own computer
```{r,tidy=TRUE}
#Get data
load_data_from_scratch=FALSE

file_names=dir(path="~/Desktop/Grad_School/2016-2017_Spring/Amin/Homework/HW 2/FIN580")
setwd("~/Desktop/Grad_School/2016-2017_Spring/Amin/Homework/HW 2/FIN580")
#file_names=dir(path="~/Desktop/Grad_School/2016-2017_Spring/Amin/Homework/HW 2/Data")
#setwd("~/Desktop/Grad_School/2016-2017_Spring/Amin/Homework/HW 2/Data")
period=288 #Only using daily vols for this assignment

if(load_data_from_scratch==TRUE){
 for(i in 1:length(file_names)){
  temp=read.csv(file_names[i],header = TRUE)
  n=ncol(temp)
  if(i==1){
    data=temp[,n]
  }
  else{
    data=cbind(data,temp[,n])
  }
 } 
  
#Change data to log returns
temp=diff(log(data),lag=period)
data=data[-1,]
data=temp
}


#Save data to .RData file 
if(load_data_from_scratch==FALSE){
  load("data.RData")
}
save(data,file="data.RData")

```

Get volatilities, annualize, and take the log. Fix windows.
```{r,tidy=TRUE}
period=288 #Only using daily vols for this assignment
get_vol=function(time_series,start_i,end_i){
  use_data=time_series[start_i:end_i,]
  sds=apply(use_data,2,sd)*sqrt(period*252)
  return(sds)
}
vols=data.frame(matrix(NA,ncol = ncol(data),nrow=nrow(data)/period))
vols=sapply(seq(1,nrow(data)-period,period),function(x) {get_vol(data,x,x+288)})
#Transpose to keep columns and currencies
vols=t(vols)
#Change to log vols
vols=log(vols)


save(vols,file="vols.RData")
load("vols.RData")

```

Here we split our data into training and test. Since we use VAR models we will split our data sequentially. We will aim to use 2/3 of the data for the training set, and 1/3 for the testing.
```{r,tidy=TRUE}
split=floor(nrow(vols)*(3.5/5))
train=vols[1:split,]
test=vols[(split+1):nrow(vols),]
save(train,file="train.RData")
save(test,file="test.RData")
```


General functions to be used across models. 
```{r,tidy=TRUE}
#Since the R lag function doesn't pad with NAs - we write our own lag function
lagpad = function(x, k) {
    if (!is.vector(x)) 
        stop('x must be a vector')
    if (!is.numeric(x)) 
        stop('x must be numeric')
    if (!is.numeric(k))
        stop('k must be numeric')
    if (1 != length(k))
        stop('k must be a single number')
    c(rep(NA, k), x)[1 : length(x)] 
}
#Get lagged data for autoreggressive models
get_lagged_data=function(time_series,max_lag){
  xs=matrix(NA,nrow=length(time_series),ncol=max_lag)
  xs[,1]=lagpad(time_series,1)
  if(max_lag>1){
    xs[,2]=lagpad(time_series,2)
  }
  if(max_lag>2){
    xs[,3]=lagpad(time_series,3)
  }
  return(na.omit(xs))
}
```

VAR model (non lasso).
```{r,tidy=TRUE}
ps=c(1,2,3)
colnames(train)=seq(1,ncol(train),1) #VAR cares about col names for some reason
fit_VAR=function(p){
  model=VAR(train,p=p)
  pred=predict(model,n.ahead=nrow(train))
  all=pred[[1]]
  data=all[[1]][,1]
  for(i in 2:ncol(test)){
    temp=all[[i]][,1]
    data=cbind(data,temp)
  }
  errs=(data-train)^2
  return(errs)
}

var_mses=list()
for(i in 1:length(ps)){
  var_mses[[i]]=fit_VAR(i)
}

#Plotting
plot_data=matrix(NA,nrow=3,ncol=ncol(test))
for(i in 1:ncol(test)){
  plot_data[1,]=mean(var_mses[[1]][,i])
  plot_data[2,]=mean(var_mses[[2]][,i])
  plot_data[3,]=mean(var_mses[[3]][,i])
}

matplot(plot_data,main="VAR model MSE by lag",xlab="Lag",ylab="MSE")
legend("bottom",legend=names,col=1:9,pch=1,ncol = 3,xpd = TRUE, text.width=c(0.4,0.4,0.4,0.4))
dev.copy(pdf,"Var model MSE by lag.pdf")
dev.off()

```
LASSO described in [1].  You are expected to run one regression with the whole training history ( rst 3.5 years) for each λ. Run the regressions with different λs and plot the  in sample  Mean Square Error (MSE) with respect to the λ. Use the best λ (corresponds to the lowest MSE) in the test sample to report the  nal evaluation numbers.
```{r,tidy=TRUE}
#Available lags
ps=c(1,2,3)

#VAR lasso
lams_opt_VAR_l=c()
fit_VAR_lasso=function(p){
  model=constructModel(data.matrix(train),p=p,"Basic",gran=c(50,10),h=1)
  res=cv.BigVAR(model)
  o_lam=res@OptimalLambda
  mse_in=res@InSampMSFE
  name=paste("VAR Lasso Results Lag",p)
  #Plot and save as pdf
  plot(res)
  title(main = name)
  #Save as pdf 
  dev.copy(pdf,paste(name,".pdf",sep=""))
  dev.off()
  return(list(o_lam,mse_in))
}

for(i in 1:length(ps)){
  temp=fit_VAR_lasso(i)
  lams_opt_VAR_l=append(lams_opt_VAR_l,temp[[1]])
}

#Expanding window fits of VAR lasso models using the optimal lambda to get 1 step predictions
#Prediction function - returns MSE for 1 prediction over all currencies
pred_var_lasso=function(data,lam,p,ans){
  model=constructModel(data.matrix(data),p=p,"Basic",ownlambdas=TRUE,gran=lam,h=1,verbose=FALSE)
  res=cv.BigVAR(model)
  pred=t(predict(res,1))
  return(mean((matrix(ans,nrow=1)-pred)^2))
}

#Returns more detailed statistics - currency by currency
pred_var_lasso_det=function(data,lam,p,ans){
  model=constructModel(data.matrix(data),p=p,"Basic",ownlambdas=TRUE,gran=lam,h=1,verbose=FALSE)
  res=cv.BigVAR(model)
  pred=t(predict(res,1))
  return((matrix(ans,nrow=1)-pred)^2)
}

#Non detailed
temp=seq(1,nrow(test)-2,1)
mses_var_lasso=data.frame(matrix(NA,nrow=(length(temp)+1),ncol=3))

for(i in 1:length(ps)){
  lam=lams_opt_VAR_l[i]
  mses_var_lasso[1,i]=pred_var_lasso(train,lam,i,test[1,])
  mses_var_lasso[2:nrow(mses_var_lasso),i]=sapply(temp,function(x){print(x) 
    pred_var_lasso(rbind(train,test[1:(x-1),]),lam,i,test[x+1,])})
}

save(mses_var_lasso,file="mses_var_lasso.RData")
load("mses_var_lasso.RData")

#Plot and sae
plot(colMeans(mses_var_lasso),main="VAR Lasso Model Training MSE by Lag",xlab="Lag",ylab="MSE",pch=24,col=2)
dev.copy(pdf,"var_lasso_test_mse.pdf")
dev.off()

#Detailed

mse_var_lasso_det_data=list()

mses_var_lasso_det_1=data.frame(matrix(NA,nrow=(length(temp)+1),ncol=9))
mses_var_lasso_det_2=data.frame(matrix(NA,nrow=(length(temp)+1),ncol=9))
mses_var_lasso_det_3=data.frame(matrix(NA,nrow=(length(temp)+1),ncol=9))

i=1
lam=lams_opt_VAR_l[i]
mses_var_lasso_det_1[1,]=pred_var_lasso_det(train,lam,i,test[1,])
mses_var_lasso_det_1[2:nrow(mses_var_lasso_det),]=sapply(temp,function(x){pred_var_lasso_det(rbind(train,test[1:(x-1),]),lam,i,test[x+1,])})
i=2
lam=lams_opt_VAR_l[i]
mses_var_lasso_det_2[1,]=pred_var_lasso_det(train,lam,i,test[1,])
mses_var_lasso_det_2[2:nrow(mses_var_lasso_det),]=sapply(temp,function(x){pred_var_lasso_det(rbind(train,test[1:(x-1),]),lam,i,test[x+1,])})
i=3
lam=lams_opt_VAR_l[i]
mses_var_lasso_det_3[1,]=pred_var_lasso_det(train,lam,i,test[1,])
mses_var_lasso_det_3[2:nrow(mses_var_lasso_det),]=sapply(temp,function(x){pred_var_lasso_det(rbind(train,test[1:(x-1),]),lam,i,test[x+1,])})
  
 

save(mses_var_lasso_det,file="mses_var_lasso_det.RData")

mses_var_lasso_det_df=data.frame(matrix(NA,ncol=9*3,nrow=length(mse_var_lasso_det_data)/3))
for(i in  1:length(mse_var_lasso_det_data)/3){
  for(j in 1:(3*9)){
    mses_var_lasso_det_df[i,j]=mse_var_lasso_det_data[i+j-1]
  }
}

names=c("AUD","CAD","CHF","EUR","GBP","JPY","NOK","NZD","SEK")

barplot(colMeans(mses_var_lasso_det_1),pch=1,col=1:9,main="Average MSE by currency for 1-lag VAR lasso",ylab="Ave MSE by currency",names.arg=names,cex.names=0.8)
dev.copy(pdf,"var_lasso_mse_curr.pdf")
dev.off()

barplot(colMeans(mses_var_lasso_det_2),pch=1,col=1:9,main="Average MSE by currency for 2-lag VAR lasso",ylab="Ave MSE by currency",names.arg=names,cex.names=0.8)
dev.copy(pdf,"var_lasso_mse_curr2.pdf")
dev.off()


```

$\textbf{Ordered lasso implementation}$
Here we fit the ordered LASSO for extra credit. 
```{r,tidy=TRUE}

#Fits lasso based on lag 
#Can also do ordered lasso based on order parameter
fit_lasso=function(col_i,max_lag,lam,order){
  ts=train[,col_i]
  ts=ts[which(ts!=0)]
  y=matrix(ts[(max_lag+1):length(ts)])
  x=get_lagged_data(ts,max_lag)
  model=orderedLasso(x,y,lambda = lam,strongly.ordered = order)
  pred_train=predict(model,x)$yhat
  new_ts=test[,col_i]
  new_ts=new_ts[which(new_ts!=0)]
  new_y=matrix(new_ts[(max_lag+1):length(new_ts)])
  new_x=get_lagged_data(new_ts,max_lag)
  pred_test=predict(model,new_x)$yhat
  return(list(mse(pred_train,y),mse(pred_test,new_y)))
}

#Getting range of lambdas
lam_1=0
#Get lam_L - maximum lambda, shrinks all parameters to 0 - based on most complex model (p=3)
currs=seq(1,ncol(train),1)
max_lag=max(ps)
get_max_L=function(col_i){
  ts=train[,col_i]
  ts=ts[which(ts!=0)]
  y=matrix(ts[(max_lag+1):length(ts)])
  x=get_lagged_data(ts,3)
  cv=cv.glmnet(x,y)
  return(cv$lambda[1])
}
max_L=sapply(currs,get_max_L)
lam_L=max(max_L)
lams=seq(0,lam_L,length.out = 10)

#Mses is a list - train first, test second
populate_tables=function(p,order){
  mse_train=matrix(NA,ncol=ncol(vols),nrow=length(lams))
  mse_test=matrix(NA,ncol=ncol(vols),nrow=length(lams))
  for(i in 1:ncol(mse_train)){
  temp=sapply(lams,function(x) fit_lasso(i,p,x,order))
  mse_train[,i]=matrix(unlist(temp[1,]),ncol=1)
  mse_test[,i]=matrix(unlist(temp[2,]),ncol=1)
  }
  return(list(mse_train,mse_test))
}

#Getting all MSE tables for ordered lasso - note we exclude 1lag as this is not a VAR model
for(i in 2:length(ps)){
  p=ps[i]
  name_train=paste("train_ordered_",p,sep="")
  name_test=paste("test_ordered_",p,sep="")
  temp=populate_tables(p,TRUE)
  assign(name_train,temp[1])
  assign(name_test,temp[2])
}
```

Plot MSEs by lambda for ordered Lasso.
```{r,tidy=TRUE}
#Get average MSEs by lambda (for plotting)
ave_mse_train=matrix(1,nrow=length(lams),ncol=length(ps))
ave_mse_test=matrix(1,nrow=length(lams),ncol=length(ps))

for(i in 1:length(ps)){
  name_train=paste("train_ordered_",p,sep="")
  name_test=paste("test_ordered_",p,sep="")
  use_train=matrix(unlist(get(name_train)),ncol=ncol(vols))
  use_test=matrix(unlist(get(name_test)),ncol=ncol(vols))
  ave_mse_train[,i]=rowMeans(use_train)
  ave_mse_test[,i]=rowMeans(use_test)
}

#PLotting
matplot(lams,ave_mse_train,type=c("b"),pch=1,col=1:3,main="Training MSEs by lambda",xlab="Lambdas",ylab="Ave MSE over all currencies")
legend("topright",legend=1:3,col=1:3,pch=1)
dev.copy(pdf,"Training MSES by lambda - ordered lasso")
dev.off()
  
matplot(lams,ave_mse_test,type=c("b"),pch=1,col=1:3,main="Testing MSEs by lambda",xlab="Lambdas",ylab="Ave MSE over all currencies")
legend("topright",legend=1:3,col=1:3,pch=1)
dev.copy(pdf,"Testing MSES by lambda - ordered lasso")
dev.off()

```



KNN method we’ve covered during class. Features and Labels First, you should use today’s vol t as the 1-dimensional feature space, which means $x_t = \sigma_t$. The label is just what you want to forecast, which means $y_t = \sigma_{t+1}$. Note that you can choose different k for KNN, such as k = {1, 3, 5, 10, 20, 50}.

Training Sample: For each day/week/month, we can either use expanding window, that means use all the data up to that date as training sample to find the k-nearest points, or you can use rolling window, that means, fix the look-back window when training your model, say for each day, you can always lookback
for 2 years as training sample.

KNN regression Based on your feature space and training sample, once you found your k-nearest points to
$x_t$ = $\sigma_t$, you can use the KNN regression we’ve covered in class (details can be found in course notes) to build a forecast for $y_t = \sigma_{t-1}$


```{r,tidy=TRUE}
#helper functions

#implements k-nearest neighbors regression on some time series ts, lambda is penalty for time, lookBack is number of data points to consider
#we used expanding window

comp_distance = function(x){
  return((x[1]-origin[1])^2  + lam* (x[2]-origin[2])^2)
}

get_knn = function(center,slice,k){
    origin <<- center
    dist = apply(slice,1,comp_distance)
    sorted = sort(dist,decreasing=F)[1:k]
    indicies = sapply(seq(1,k), function(x) {which(dist == sorted[x])})
    
    #indicies = c()
    #for(j in 1:k){
    # indicies[j] = which(dist == sorted[j]) 
    #}
    slice[length(slice) + 1] = origin[1]
    return(slice[indicies+1]) #get what happened the next day
}
#function for fitting on training data
knn_regression = function(ts,k,lambda,lookBack){ 
  START = 100
  lam <<- lambda
  time = c(seq(1,length(ts),1))
  ts = cbind(ts, time)
  regressors = matrix(nrow = nrow(ts)-START,ncol = k)
  y = c()
  for(i in START:(nrow(ts)-1)){
    lag = min(i,lookBack)
    regressors[i-START+1,] = get_knn(ts[i,],ts[(i-lag):(i-1),],k)
    y[i-START+1] = ts[i+1,1] #next days vol 
  }
  model = lm(y ~ 0 + regressors, data = as.data.frame(regressors))
  assign(paste("model_",k,sep = ""), model, globalenv()) #no intercept!
  #assign(paste("model_", currency, "_",k,sep = ""), model, globalenv()) #no intercept!
  return( 1.0/length(y) *   sum((model$fitted.values - y)^2)) #training mse
}

#testing diagnostics
knn_test_diag = function(ts,k,lambda,lookBack){
  START = 910
  lam <<- lambda
  time = c(seq(1,length(ts),1))
  ts = cbind(ts, time)
  regressors = matrix(nrow = nrow(ts)-START,ncol = k)
  y = c()
  for(i in START:(nrow(ts)-1)) {
    lag = min(i,lookBack)
    get_knn(ts[i,],ts[(i-lag):(i-1),],k)
    regressors[i-START+1,] = get_knn(ts[i,],ts[(i-lag):(i-1),],k)
    y[i-START+1] = ts[i+1,1] #next days vol 
  } 
  #model = get(paste("model_",curr,"_",k,sep = ""))
  model = get(paste("model_",k,sep = ""))
  fitted = model$coefficients %*% t(as.data.frame(regressors))
  return( 1.0/length(y) * sum((fitted - y)^2)) #testing mse
}


```



Fit knn regression and choose k on testing
```{r,tidy = TRUE}
ks= seq(1,20,1)
tot_data = rbind(train,test)

model_coef = vector("list", ncol(train))
training_mse = matrix(nrow = length(ks),ncol = ncol(train))
testing_mse = matrix(nrow = length(ks),ncol = ncol(tot_data))

for(curr in 1:ncol(train)){
  model_coef[[curr]] = matrix(0,nrow = length(ks),ncol = length(ks))
  for(k in ks){
    #currency <<- curr
    training_mse[k,curr] = knn_regression(train[,curr],k,0,nrow(tot_data))
    testing_mse[k,curr] = knn_test_diag(tot_data[,curr],k,0,nrow(tot_data))
    model_coef[[curr]][k,1:k] = get(paste("model_",k,sep=""))$coefficients
  }
}

k_opt = c()
for(i in 1:ncol(train)){
  k_opt[i] = which.min(testing_mse[,i])
}

tot_mse = rep(0,20)
for(i in 1:ncol(train)){
  tot_mse = tot_mse+ testing_mse[,i]
}


k_robust = which.min(tot_mse)

```



Plotting diagnostics
```{r, tidy = TRUE}

currencies = c("AUD","CAD","CHF","EUR","GBP","JPY","NOK","NZD","SEK")
for(i in 1:ncol(train)){
  name = paste("Training MSE v k for for ", currencies[i], sep = "")
  plot(ks,training_mse[,i],main = name, ylab = "MSE", xlab = "k", pch=21,  bg="blue")
  dev.copy(pdf,paste(name,".pdf",sep=""))
  dev.off()
  name = paste("Testing MSE v k for ", currencies[i], sep = "")
  plot(ks,testing_mse[,i],main = name, ylab = "MSE", xlab = "k", pch=21,  bg="blue")
  points(k_opt[i],tot_mse[k_opt[i]], bg = 'red')
  dev.copy(pdf,paste(name,".pdf",sep=""))
  dev.off()
}

name = "Aggregate Testing MSE v k"
plot(ks,tot_mse, main = name, ylab = "MSE", xlab = "k",, pch=21,  bg="blue")
points(k_robust,tot_mse[k_robust], pch=21,  bg="red")
dev.copy(pdf,paste(name,".pdf",sep=""))
dev.off()

#plot the coefficients, shows that knn is not good for this purpose
for(i in 1:ncol(train)){
  for(k in ks){
    name =  paste("Coefficients for ", k, "-NN for ", currencies[i], sep = "")
    plot(ks[1:k],model_coef[[i]][k,1:k], main =name, ylab = "Coefficient Size", xlab = "ith nearest neighbor", pch=21,  bg="blue")
    dev.copy(pdf,paste(name,".pdf",sep=""))
    dev.off()
    
  }
}
for(i in 1:ncol(train)){
  name =  paste("Sum of Coefficients v k for ", currencies[i], sep = "")
  sum_coef =  apply(model_coef[[i]],1,sum)
  plot(ks[1:k],sum_coef, main =name, ylab = "Sum", xlab = "k", pch=21,  bg="blue")
  dev.copy(pdf,paste(name,".pdf",sep=""))
  dev.off()
}

```



Research problem, added cost to knn, grid search over c to find optimal cost

```{r, tidy = TRUE}

#cost implementation
cs = seq(0,.4, .4/10) #for c>.4 it takes the nearest k as cost is too high
ks = c(1,3,6,9,12,16,20)
c_training_mse = vector("list", ncol(train))
c_testing_mse = vector("list", ncol(train))
c_model_coef = vector("list", ncol(train))

for(curr in 1:ncol(train)){
    c_testing_mse[[curr]] = matrix(0,nrow = length(ks),ncol = length(cs))  
    c_training_mse[[curr]] = matrix(0,nrow = length(ks),ncol = length(cs))
    c_model_coef[[curr]] = vector("list", length(cs))
  for(i in 1:length(cs)){
     c_model_coef[[curr]][[i]] = matrix(0,nrow = length(ks),ncol = ks[length(ks)])
     
    for(j in 1:length(ks)){
      c_training_mse[[curr]][j,i] = knn_regression(train[,curr],ks[j],cs[i],nrow(tot_data))
      c_testing_mse[[curr]][j,i] = knn_test_diag(tot_data[,curr],ks[j],cs[i],nrow(tot_data))
      c_model_coef[[curr]][[i]][j,1:ks[j]] = get(paste("model_",ks[j],sep=""))$coefficients 
    }
  }
}

```


Plotting and diagnostics for cost
```{r,tidy = TRUE}

#find optimal cost
tot_mse_cost = matrix(0,nrow = length(ks),ncol = length(cs))
for(i in 1:ncol(train)){
  tot_mse_cost = tot_mse_cost + c_testing_mse[[i]]
}



opt_ck = as.data.frame(which(tot_mse_cost == min(tot_mse_cost), arr.ind = TRUE))
colnames(opt_ck) = c("k","c")
opt_ck$k = ks[opt_ck$k]
opt_ck$c = cs[opt_ck$c]

tot_mse_k = c()
for(i in 1:length(ks)){
  tot_mse_k[i] = min(tot_mse_cost[i,])
}

name = "Aggregate Testing MSE v k with time factor"
plot(ks,tot_mse_k, main = name, xlab = "k",ylab = "MSE", pch=21,  bg="blue")
points(opt_ck$k,tot_mse_k[which(ks == opt_ck$k)], pch = 21, bg="red")
dev.copy(pdf,paste(name,".pdf",sep=""))
dev.off()

#make chart

results = matrix(nrow = ncol(train),ncol = 2 )
colnames(results) = c("mse","mse_time") 
rownames(results) = currencies

for(i in 1:ncol(train)){
  results[i,1] = min(testing_mse[,i])
  results[i,2] = min(c_testing_mse[[i]])
}

pdf("knn_results.pdf", height=11, width=8.5)
grid.table(results)
dev.off()

```



#note code has revolving window implementation, can explore for future research