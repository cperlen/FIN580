---
title: "Gnedin_Perlen_Liu_FIN580_HW2"
author: "Gnedin_Perlen_Liu"
date: "March 8, 2017"
output: pdf_document
---

```{r,tidy=TRUE}
#Setup 
library(xlsx)
library(forecast)
library(tseries)
library(stats)
library(car)
library(glmnet)
library(miscTools)
library(Metrics)
library(knitr)
library(class)
library(BigVAR)
library(orderedLasso)
library(BioPhysConnectoR)
#Set seed
set.seed(666)

```

Read in data. There might some days the price remains constant. You should check the data and delete those days, since the estimate of volatility will be zero
```{r,tidy=TRUE}
#Get data
load_data_from_scratch=FALSE

file_names=dir(path="~/Desktop/Grad_School/2016-2017_Spring/Amin/Homework/HW 2/FIN580")
setwd("~/Desktop/Grad_School/2016-2017_Spring/Amin/Homework/HW 2/FIN580")
#file_names=dir(path="~/Desktop/Grad_School/2016-2017_Spring/Amin/Homework/HW 2/Data")
#setwd("~/Desktop/Grad_School/2016-2017_Spring/Amin/Homework/HW 2/Data")
period=288 #Only using daily vols for this assignment

if(load_data_from_scratch==TRUE){
 for(i in 1:length(file_names)){
  temp=read.csv(file_names[i],header = TRUE)
  n=ncol(temp)
  if(i==1){
    data=temp[,n]
  }
  else{
    data=cbind(data,temp[,n])
  }
 } 
  
#Change data to log returns
temp=diff(log(data),lag=period)
data=data[-1,]
data=temp
}


#Save data to .RData file 
if(load_data_from_scratch==FALSE){
  load("data.RData")
}
save(data,file="data.RData")

#TBU - potentially remove in model fitting stage and ignore here
remove_constants=function(time_series){
  new_series=time_series
  for(i in 1:length(time_series)){
    if(new_series[i+1]==new_series[i]){
      
    }
  }
}
```

Get volatilities, annualize, and take the log. Fix windows.
```{r,tidy=TRUE}
period=288 #Only using daily vols for this assignment
get_vol=function(time_series,start_i,end_i){
  use_data=time_series[start_i:end_i,]
  sds=apply(use_data,2,sd)*sqrt(period*252)
  return(sds)
}
vols=data.frame(matrix(NA,ncol = ncol(data),nrow=nrow(data)/period))
vols=sapply(seq(1,nrow(data)-period,period),function(x) {get_vol(data,x,x+288)})
#Transpose to keep columns and currencies
vols=t(vols)
#Change to log vols
vols=log(vols)

save(vols,file="vols.RData")
```

Here we split our data into training and test. Since we use VAR models we will split our data sequentially. We will aim to use 2/3 of the data for the training set, and 1/3 for the testing.
```{r,tidy=TRUE}
split=floor(nrow(vols)*(3.5/5))
train=vols[1:split,]
test=vols[(split+1):nrow(vols),]
save(train,file="train.RData")
save(test,file="test.RData")
```


General functions. 

```{r,tidy=TRUE}
#Since the R lag function doesn't pad with NAs - we write our own lag function
lagpad = function(x, k) {
    if (!is.vector(x)) 
        stop('x must be a vector')
    if (!is.numeric(x)) 
        stop('x must be numeric')
    if (!is.numeric(k))
        stop('k must be numeric')
    if (1 != length(k))
        stop('k must be a single number')
    c(rep(NA, k), x)[1 : length(x)] 
}
#Get lagged data for autoreggressive models
get_lagged_data=function(time_series,max_lag){
  xs=matrix(NA,nrow=length(time_series),ncol=max_lag)
  xs[,1]=lagpad(time_series,1)
  if(max_lag>1){
    xs[,2]=lagpad(time_series,2)
  }
  if(max_lag>2){
    xs[,3]=lagpad(time_series,3)
  }
  return(na.omit(xs))
}

```
LASSO described in [1]. You should also use log volatility instead of volatility to address the negativity problem in regression. Compare the two results. You should divide your data into training and test sample, so that you can find the optimized in training set and use it for the test sample.
```{r,tidy=TRUE}
#Available lags
ps=c(1,2,3)
#Getting range of lambdas
lam_1=0
#Get lam_L - maximum lambda, shrinks all parameters to 0 - based on most complex model (p=3)
currs=seq(1,ncol(train),1)
get_max_L=function(col_i){
  ts=train[,col_i]
  ts=ts[which(ts!=0)]
  y=matrix(ts[(max_lag+1):length(ts)])
  x=get_lagged_data(ts,3)
  cv=cv.glmnet(x,y)
  return(cv$lambda[1])
}
max_L=sapply(currs,get_max_L)
lam_L=max(max_L)
lams=seq(0,lam_L,length.out = 10)

#Fits lasso based 
#Resource http://www.wbnicholson.com/BigVAR.html
#struct=none is lasso penalty
fit_lasso=function(col_i,max_lag,lam){
  ts=train[,col_i]
  ts=ts[which(ts!=0)]
  y=matrix(ts[(max_lag+1):length(ts)])
  x=get_lagged_data(ts,max_lag)
  model=orderedLasso(x,y,lambda = lam,strongly.ordered = FALSE)
  pred_train=predict(model,x)$yhat
  new_ts=test[,col_i]
  new_ts=new_ts[which(new_ts!=0)]
  new_y=matrix(new_ts[(max_lag+1):length(new_ts)])
  new_x=get_lagged_data(new_ts,max_lag)
  pred_test=predict(model,new_x)$yhat
  return(list(mse(pred_train,y),mse(pred_test,new_y)))
}
#Mses is a list - train first, test second
mse_train=matrix(NA,ncol=ncol(vols),nrow=length(lams))
mse_test=matrix(NA,ncol=ncol(vols),nrow=length(lams))

populate_tables=function(p){
  for(i in 1:ncol(mse_train)){
  temp=sapply(lams,function(x) fit_lasso(i,p,x))
  print(unlist(temp[1,]))
  mse_train[,i]=matrix(unlist(temp[1,]),ncol=1)
  mse_test[,i]=matrix(unlist(temp[2,]),ncol=1)
  }
}

populate_tables(3)




```


KNN method we’ve covered during class. Features and Labels First, you should use today’s vol t as the 1-dimensional feature space, which means $x_t$ = t. The label is just what you want to forecast, which means $y_t$ = t+1. Note that you can choose different k for KNN, such as k = {1, 3, 5, 10, 20, 50}.
Training Sample: For each day/week/month, we can either use expanding window, that means use all the data up to that date as training sample to find the k-nearest points, or you can use rolling window, that means, fix the look-back window when training your model, say for each day, you can always lookback
for 2 years as training sample.

KNN regression Based on your feature space and training sample, once you found your k-nearest points to
$x_t$ = t, you can use the KNN regression we’ve covered in class (details can be found in course notes) to build a forecast for $y_t$ = t+1.
```{r,tidy=TRUE}
ks=c(1,3,5,10,20)
lookback_knn=period*252*2 #Two years
accuracy_knn=c()
knn_prediction=function(i,k){
  train_ts=train[i-lookback_knn-1:i-1]
  train_labels=train[i-lookback_knn:i]
  knn=knn(train_ts,test,train_labels,k=k)
  return(mean(knn==test[-1]))
}


```