---
title: "Gnedin_Perlen_Liu_FIN580_HW2"
author: "Gnedin_Perlen_Liu"
date: "March 8, 2017"
output: pdf_document
---

```{r,tidy=TRUE}
#Setup 
library(xlsx)
library(forecast)
library(tseries)
library(stats)
library(car)
library(glmnet)
library(miscTools)
library(Metrics)
library(knitr)
library(class)
library(BigVAR)
library(orderedLasso)
library(BioPhysConnectoR)
#Set seed
set.seed(666)

```

Read in data. There might some days the price remains constant. You should check the data and delete those days, since the estimate of volatility will be zero
```{r,tidy=TRUE}
#Get data
load_data_from_scratch=FALSE

file_names=dir(path="~/Desktop/Grad_School/2016-2017_Spring/Amin/Homework/HW 2/FIN580")
setwd("~/Desktop/Grad_School/2016-2017_Spring/Amin/Homework/HW 2/FIN580")
#file_names=dir(path="~/Desktop/Grad_School/2016-2017_Spring/Amin/Homework/HW 2/Data")
#setwd("~/Desktop/Grad_School/2016-2017_Spring/Amin/Homework/HW 2/Data")
period=288 #Only using daily vols for this assignment

if(load_data_from_scratch==TRUE){
 for(i in 1:length(file_names)){
  temp=read.csv(file_names[i],header = TRUE)
  n=ncol(temp)
  if(i==1){
    data=temp[,n]
  }
  else{
    data=cbind(data,temp[,n])
  }
 } 
  
#Change data to log returns
temp=diff(log(data),lag=period)
data=data[-1,]
data=temp
}


#Save data to .RData file 
if(load_data_from_scratch==FALSE){
  load("data.RData")
}
save(data,file="data.RData")

#TBU - potentially remove in model fitting stage and ignore here
remove_constants=function(time_series){
  new_series=time_series
  for(i in 1:length(time_series)){
    if(new_series[i+1]==new_series[i]){
      
    }
  }
}
```

Get volatilities, annualize, and take the log. Fix windows.
```{r,tidy=TRUE}
period=288 #Only using daily vols for this assignment
get_vol=function(time_series,start_i,end_i){
  use_data=time_series[start_i:end_i,]
  sds=apply(use_data,2,sd)*sqrt(period*252)
  return(sds)
}
vols=data.frame(matrix(NA,ncol = ncol(data),nrow=nrow(data)/period))
vols=sapply(seq(1,nrow(data)-period,period),function(x) {get_vol(data,x,x+288)})
#Transpose to keep columns and currencies
vols=t(vols)
#Change to log vols
vols=log(vols)

save(vols,file="vols.RData")
```

Here we split our data into training and test. Since we use VAR models we will split our data sequentially. We will aim to use 2/3 of the data for the training set, and 1/3 for the testing.
```{r,tidy=TRUE}
split=floor(nrow(vols)*(3.5/5))
train=vols[1:split,]
test=vols[(split+1):nrow(vols),]
save(train,file="train.RData")
save(test,file="test.RData")
```


General functions to be used across models. 

```{r,tidy=TRUE}
#Since the R lag function doesn't pad with NAs - we write our own lag function
lagpad = function(x, k) {
    if (!is.vector(x)) 
        stop('x must be a vector')
    if (!is.numeric(x)) 
        stop('x must be numeric')
    if (!is.numeric(k))
        stop('k must be numeric')
    if (1 != length(k))
        stop('k must be a single number')
    c(rep(NA, k), x)[1 : length(x)] 
}
#Get lagged data for autoreggressive models
get_lagged_data=function(time_series,max_lag){
  xs=matrix(NA,nrow=length(time_series),ncol=max_lag)
  xs[,1]=lagpad(time_series,1)
  if(max_lag>1){
    xs[,2]=lagpad(time_series,2)
  }
  if(max_lag>2){
    xs[,3]=lagpad(time_series,3)
  }
  return(na.omit(xs))
}
```

VAR model.
```{r,tidy=TRUE}
ps=c(1,2,3)
fit_VAR=function(p){
  model=constructModel(train,p,"Basic",gran=c(50,10))
  pred_train=predict(model,train)
  pred_test=predict(mode,test)
  return(list(mse(pred_train,train),mse(pred_test,test)))
}

```
LASSO described in [1].  You are expected to run one regression with the whole training history ( rst 3.5 years) for each λ. Run the regressions with di erent λs and plot the  in sample  Mean Square Error (MSE) with respect to the λ. Use the best λ (corresponds to the lowest MSE) in the test sample to report the  nal evaluation numbers.
```{r,tidy=TRUE}
#Available lags
ps=c(1,2,3)
#Getting range of lambdas
lam_1=0
#Get lam_L - maximum lambda, shrinks all parameters to 0 - based on most complex model (p=3)
currs=seq(1,ncol(train),1)
get_max_L=function(col_i){
  ts=train[,col_i]
  ts=ts[which(ts!=0)]
  y=matrix(ts[(max_lag+1):length(ts)])
  x=get_lagged_data(ts,3)
  cv=cv.glmnet(x,y)
  return(cv$lambda[1])
}
max_L=sapply(currs,get_max_L)
lam_L=max(max_L)
lams=seq(0,lam_L,length.out = 10)

#Fits lasso based 
#Can also do ordered lasso based on order parameter
fit_lasso=function(col_i,max_lag,lam,order){
  ts=train[,col_i]
  ts=ts[which(ts!=0)]
  y=matrix(ts[(max_lag+1):length(ts)])
  x=get_lagged_data(ts,max_lag)
  model=orderedLasso(x,y,lambda = lam,strongly.ordered = order)
  pred_train=predict(model,x)$yhat
  new_ts=test[,col_i]
  new_ts=new_ts[which(new_ts!=0)]
  new_y=matrix(new_ts[(max_lag+1):length(new_ts)])
  new_x=get_lagged_data(new_ts,max_lag)
  pred_test=predict(model,new_x)$yhat
  return(list(mse(pred_train,y),mse(pred_test,new_y)))
}
#Mses is a list - train first, test second
mse_train=matrix(NA,ncol=ncol(vols),nrow=length(lams))
mse_test=matrix(NA,ncol=ncol(vols),nrow=length(lams))

populate_tables=function(p,order){
  for(i in 1:ncol(mse_train)){
  temp=sapply(lams,function(x) fit_lasso(i,p,x,order))
  print(unlist(temp[1,]))
  mse_train[,i]=matrix(unlist(temp[1,]),ncol=1)
  print(mse_train[,i])
  mse_test[,i]=matrix(unlist(temp[2,]),ncol=1)
  }
  return(list(mse_train,mse_test))
}

#Getting all MSE tables
populate_tables(1)

for(i in 1:length(ps)){
  p=ps[i]
  name_train=paste("train_",p,sep="")
  name_test=paste("test_",p,sep="")
  temp=populate_tables(p,FALSE)
  assign(name_train,temp[1])
  assign(name_test,temp[2])
}
```

Plot MSEs by lambda.
```{r,tidy=TRUE}
#Get average MSEs by lambda (for plotting)
ave_mse_train=matrix(1,nrow=length(lams),ncol=length(ps))
ave_mse_test=matrix(1,nrow=length(lams),ncol=length(ps))
for(i in 1:length(ps)){
  name_train=paste("train_",p,sep="")
  name_test=paste("test_",p,sep="")
  use_train=matrix(unlist(get(name_train)),ncol=ncol(vols))
  use_test=matrix(unlist(get(name_test)),ncol=ncol(vols))
  ave_mse_train[,i]=rowMeans(use_train)
  ave_mse_test[,i]=rowMeans(use_test)
}

#PLotting
matplot(lams,ave_mse_train,type=c("b"),pch=1,col=1:3,main="Training MSEs by lambda",xlab="Lambdas",ylab="Ave MSE over all currencies")
legend("topright",legend=1:3,col=1:3,pch=1)
matplot(lams,ave_mse_test,type=c("b"),pch=1,col=1:3,main="Testing MSEs by lambda",xlab="Lambdas",ylab="Ave MSE over all currencies")
legend("topright",legend=1:3,col=1:3,pch=1)

```

$\textbf{Extra Points:}$ Ordered lasso implementation.
```{r,tide=TRUE}
for(i in 1:length(ps)){
  p=ps[i]
  name_train=paste("train_ord_",p,sep="")
  name_test=paste("test_ord_",p,sep="")
  temp=populate_tables(p,TRUE)
  assign(name_train,temp[1])
  assign(name_test,temp[2])
}

```

KNN method we’ve covered during class. Features and Labels First, you should use today’s vol t as the 1-dimensional feature space, which means $x_t$ = t. The label is just what you want to forecast, which means $y_t$ = t+1. Note that you can choose different k for KNN, such as k = {1, 3, 5, 10, 20, 50}.
Training Sample: For each day/week/month, we can either use expanding window, that means use all the data up to that date as training sample to find the k-nearest points, or you can use rolling window, that means, fix the look-back window when training your model, say for each day, you can always lookback
for 2 years as training sample.

KNN regression Based on your feature space and training sample, once you found your k-nearest points to
$x_t$ = t, you can use the KNN regression we’ve covered in class (details can be found in course notes) to build a forecast for $y_t$ = t+1.
```{r,tidy=TRUE}
ks=c(1,3,5,10,20)
lookback_knn=period*252*2 #Two years
accuracy_knn=c()
knn_prediction=function(i,k){
  train_ts=train[i-lookback_knn-1:i-1]
  train_labels=train[i-lookback_knn:i]
  knn=knn(train_ts,test,train_labels,k=k)
  return(mean(knn==test[-1]))
}


```